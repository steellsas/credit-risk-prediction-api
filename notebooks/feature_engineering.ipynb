{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30d163e4",
   "metadata": {},
   "source": [
    "# Feature Engineering & Aggregation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647e8fd0",
   "metadata": {},
   "source": [
    "# Feature Engineering & Aggregation Overview\n",
    "\n",
    "This notebook outlines the full pipeline for transforming raw credit data into a structured, feature-rich dataset for modeling. It includes data cleaning, feature engineering, aggregation across multiple sources, and final merging into a unified applicant-level view.\n",
    "\n",
    "\n",
    "## Main Datasets\n",
    "\n",
    "- **`application_train` / `application_test`**  \n",
    "  Core datasets containing applicant-level information and the target variable.  \n",
    "  These are merged into a single `application` DataFrame for unified processing.\n",
    "\n",
    "\n",
    "## Supporting Datasets & Relationships\n",
    "\n",
    "| Dataset                  | Key Column         | Linked To           | Description                             |\n",
    "|--------------------------|--------------------|----------------------|-----------------------------------------|\n",
    "| `bureau`                | `SK_ID_CURR`       | Application          | Credit history records per applicant    |\n",
    "| `bureau_balance`        | `SK_ID_BUREAU`     | Bureau               | Monthly status updates for bureau loans |\n",
    "| `previous_application`  | `SK_ID_CURR`       | Application          | Historical loan applications            |\n",
    "| `credit_card_balance`   | `SK_ID_PREV`       | Previous Application | Monthly credit card usage               |\n",
    "| `installments_payments`| `SK_ID_PREV`       | Previous Application | Installment payment behavior            |\n",
    "| `POS_CASH_balance`      | `SK_ID_PREV`       | Previous Application | Point-of-sale loan activity             |\n",
    "\n",
    "## Aggregation Functions\n",
    "\n",
    "Each child table is aggregated to the applicant level (`SK_ID_CURR`) using statistical summaries and behavioral indicators.\n",
    "\n",
    "### Bureau + Bureau Balance\n",
    "- Total bureau records per applicant\n",
    "- Mean/max of `DAYS_CREDIT`, `CREDIT_DAY_OVERDUE`, `AMT_CREDIT_SUM`\n",
    "- Most frequent loan status (`STATUS`)\n",
    "- Duration of bureau record (`MONTHS_BALANCE`)\n",
    "- Delinquency ratios and closure timing\n",
    "\n",
    "### Previous Applications\n",
    "- Count of previous loans\n",
    "- Approval/refusal rates\n",
    "- Mean/max of `AMT_APPLICATION`, `AMT_CREDIT`, `DAYS_DECISION`\n",
    "- Credit-to-application ratio and interest estimation\n",
    "\n",
    "### Credit Card Balance\n",
    "- Mean/max of `AMT_BALANCE`, `AMT_PAYMENT_CURRENT`\n",
    "- Drawing and payment ratios\n",
    "- Count of late payments (`SK_DPD` > 0)\n",
    "\n",
    "### Installments Payments\n",
    "- Mean/max of `AMT_INSTALMENT`, `AMT_PAYMENT`\n",
    "- Payment-to-installment ratio\n",
    "- Days past due and early payments\n",
    "- Count of late payments\n",
    "\n",
    "### POS Cash Balance\n",
    "- Mean/max of `MONTHS_BALANCE`, `SK_DPD`, `SK_DPD_DEF`\n",
    "- Total loan term (`CNT_INSTALMENT` + `CNT_INSTALMENT_FUTURE`)\n",
    "- Delinquency ratios and behavioral flags\n",
    "\n",
    "\n",
    "## Feature Engineering Functions\n",
    "\n",
    "###  From `application`:\n",
    "- `income_credit_ratio` = `AMT_INCOME_TOTAL` / `AMT_CREDIT`\n",
    "- `employment_age_ratio` = `DAYS_EMPLOYED` / `DAYS_BIRTH`\n",
    "- `external_sources_mean` = average of `EXT_SOURCE_1`, `EXT_SOURCE_2`, `EXT_SOURCE_3`\n",
    "- Interaction features: EXT_SOURCE × EMPLOYED/BIRTH\n",
    "- Credit and income comparisons: `CREDIT_DIV_ANNUITY`, `INCOME_MINUS_GOODS`\n",
    "- Behavioral flags: `APP_MISSING_COUNT`, `GOODS_PRICE_POPULAR_TIER`\n",
    "\n",
    "###  From Aggregated Tables:\n",
    "- `credit_activity_intensity` = bureau record count / age\n",
    "- `financial_stress_score` = sum of overdue amounts, late payments\n",
    "- `application_behavior_score` = frequency and approval rate of previous loans\n",
    "\n",
    "\n",
    "## 5 Final Merge & Output\n",
    "\n",
    "- All aggregated features are merged into the `application` DataFrame using `SK_ID_CURR`\n",
    "- Final dataset includes cleaned, encoded, and engineered features across all sources\n",
    "- Saved as `data_appliecation_train.parquet`, `data_appliecation_test.parquet` for downstream modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8160a282",
   "metadata": {},
   "source": [
    "##  1. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72630007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from src.utils.eda_utils import (\n",
    "    reduce_memory_usage_pl\n",
    ")\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d394ce22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before reduction: 289.57 MB\n",
      "Initial data types: Counter({Float64: 65, Int64: 41, String: 16})\n",
      "Size after reduction: 111.85 MB\n",
      "Final data types: Counter({Float32: 65, Int8: 37, Categorical(ordering='physical'): 16, Int32: 2, Int16: 2})\n",
      "Size before reduction: 45.49 MB\n",
      "Initial data types: Counter({Float64: 65, Int64: 40, String: 16})\n",
      "Size after reduction: 17.67 MB\n",
      "Final data types: Counter({Float32: 65, Int8: 36, Categorical(ordering='physical'): 16, Int32: 2, Int16: 2})\n",
      "Size before reduction: 442.60 MB\n",
      "Initial data types: Counter({Int64: 2, String: 1})\n",
      "Size after reduction: 234.32 MB\n",
      "Final data types: Counter({Int32: 1, Int8: 1, Categorical(ordering='physical'): 1})\n",
      "Size before reduction: 233.43 MB\n",
      "Initial data types: Counter({Float64: 8, Int64: 6, String: 3})\n",
      "Size after reduction: 101.28 MB\n",
      "Final data types: Counter({Float32: 8, Int32: 3, Categorical(ordering='physical'): 3, Int16: 2, Int8: 1})\n",
      "Size before reduction: 830.48 MB\n",
      "Initial data types: Counter({Float64: 5, Int64: 3})\n",
      "Size after reduction: 389.32 MB\n",
      "Final data types: Counter({Float32: 5, Int32: 2, Int16: 1})\n",
      "Size before reduction: 470.12 MB\n",
      "Initial data types: Counter({String: 16, Float64: 15, Int64: 6})\n",
      "Size after reduction: 226.18 MB\n",
      "Final data types: Counter({Categorical(ordering='physical'): 16, Float32: 15, Int32: 3, Int8: 2, Int16: 1})\n",
      "Size before reduction: 595.96 MB\n",
      "Initial data types: Counter({Int64: 5, Float64: 2, String: 1})\n",
      "Size after reduction: 240.84 MB\n",
      "Final data types: Counter({Int32: 2, Float32: 2, Int16: 2, Int8: 1, Categorical(ordering='physical'): 1})\n",
      "Size before reduction: 830.48 MB\n",
      "Initial data types: Counter({Float64: 5, Int64: 3})\n",
      "Size after reduction: 389.32 MB\n",
      "Final data types: Counter({Float32: 5, Int32: 2, Int16: 1})\n",
      "Size before reduction: 671.05 MB\n",
      "Initial data types: Counter({Float64: 15, Int64: 7, String: 1})\n",
      "Size after reduction: 308.10 MB\n",
      "Final data types: Counter({Float32: 15, Int32: 3, Int16: 3, Int8: 1, Categorical(ordering='physical'): 1})\n"
     ]
    }
   ],
   "source": [
    "application_train = pl.read_csv(\"../data/application_train.csv\")\n",
    "application_train = reduce_memory_usage_pl(application_train)\n",
    "\n",
    "application_test = pl.read_csv(\"../data/application_test.csv\")\n",
    "application_test = reduce_memory_usage_pl(application_test)\n",
    "\n",
    "bureau_bal = pl.read_csv(\"../data/bureau_balance.csv\")\n",
    "bureau_bal = reduce_memory_usage_pl(bureau_bal)\n",
    "\n",
    "bureau = pl.read_csv(\"../data/bureau.csv\")\n",
    "bureau = bureau.with_columns(\n",
    "    pl.col(\"AMT_ANNUITY\").cast(pl.Float64).alias(\"AMT_ANNUITY\")\n",
    ")\n",
    "bureau = reduce_memory_usage_pl(bureau)\n",
    "\n",
    "inst_pay_df= pl.read_csv(\"../data/installments_payments.csv\")\n",
    "inst_pay_df = reduce_memory_usage_pl(inst_pay_df)\n",
    "\n",
    "prev_app_df= pl.read_csv(\"../data/previous_application.csv\")\n",
    "prev_app_df = reduce_memory_usage_pl(prev_app_df)\n",
    "\n",
    "\n",
    "pos_cash= pl.read_csv(\"../data/POS_CASH_balance.csv\")\n",
    "pos_cash = reduce_memory_usage_pl(pos_cash)\n",
    "\n",
    "inst_pay_df= pl.read_csv(\"../data/installments_payments.csv\")\n",
    "inst_pay_df = reduce_memory_usage_pl(inst_pay_df)\n",
    "\n",
    "card_balance = pl.read_csv(\"../data/credit_card_balance.csv\")\n",
    "card_balance = reduce_memory_usage_pl(card_balance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82a6a98",
   "metadata": {},
   "source": [
    "## 2. Clean & Engineer Application Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39dcd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_applicans_data(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans and transforms a Polars DataFrame with application data.\n",
    "    Applies fixes to birth/employment days, flags anomalies, and removes invalid entries.\n",
    "    \"\"\"\n",
    "    # Convert DAYS_BIRTH to age in years\n",
    "    df = df.with_columns(\n",
    "        (pl.col(\"DAYS_BIRTH\") * -1 / 365).round().alias(\"DAYS_BIRTH\")\n",
    "    )\n",
    "\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"DAYS_EMPLOYED\").abs().alias(\"DAYS_EMPLOYED\")\n",
    "    )\n",
    "\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col(\"DAYS_EMPLOYED\") == 365243)\n",
    "          .then(None)\n",
    "          .otherwise(pl.col(\"DAYS_EMPLOYED\"))\n",
    "          .alias(\"DAYS_EMPLOYED\")\n",
    "    )\n",
    "\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"DAYS_EMPLOYED\").is_null().alias(\"YEAR_EMPLOYED_ANOM\")\n",
    "    )\n",
    "\n",
    "\n",
    "    # Remove invalid gender entries\n",
    "    df = df.filter(pl.col(\"CODE_GENDER\") != \"XNA\")\n",
    "    df = df.filter(pl.col(\"NAME_INCOME_TYPE\") != \"Maternity leave\")\n",
    "    df = df.filter(pl.col(\"NAME_FAMILY_STATUS\") != \"Unknown\")\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_new_features_application(df):\n",
    "    \"\"\"\n",
    "    Create new engineered features for the application dataframe, including ratios, products, and missing value counts.\n",
    "    Args:\n",
    "        df: polars DataFrame containing application data.\n",
    "    Returns:\n",
    "        polars DataFrame with new features added.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.with_columns(\n",
    "            pl.mean_horizontal([\n",
    "                pl.col('EXT_SOURCE_1'),\n",
    "                pl.col('EXT_SOURCE_2'),\n",
    "                pl.col('EXT_SOURCE_3')\n",
    "            ]).alias('EXT_SOURCE_MEAN')\n",
    "        )\n",
    "\n",
    "    df = df.with_columns([\n",
    "        # Missing value count\n",
    "        pl.concat_list([pl.col(c).is_null().cast(pl.Int32) for c in df.columns]).list.sum().alias('APP_MISSING_COUNT'),\n",
    "\n",
    "   \n",
    "        (pl.col('EXT_SOURCE_1') * pl.col('EXT_SOURCE_2') * pl.col('EXT_SOURCE_3')).alias('EXT_SOURCE_PRODUCT'),\n",
    "        (pl.col('EXT_SOURCE_1') * pl.col('EXT_SOURCE_2')).alias('EXT_SOURCE_1_X_2'),\n",
    "        (pl.col('EXT_SOURCE_1') * pl.col('EXT_SOURCE_3')).alias('EXT_SOURCE_1_X_3'),\n",
    "        (pl.col('EXT_SOURCE_2') * pl.col('EXT_SOURCE_3')).alias('EXT_SOURCE_2_X_3'),\n",
    "\n",
    "        # EXT_SOURCE × EMPLOYED & BIRTH\n",
    "        (pl.col('EXT_SOURCE_1') * pl.col('DAYS_EMPLOYED')).alias('EXT_SOURCE_1_EMPLOYED'),\n",
    "        (pl.col('EXT_SOURCE_2') * pl.col('DAYS_EMPLOYED')).alias('EXT_SOURCE_2_EMPLOYED'),\n",
    "        (pl.col('EXT_SOURCE_3') * pl.col('DAYS_EMPLOYED')).alias('EXT_SOURCE_3_EMPLOYED'),\n",
    "        (pl.col('EXT_SOURCE_1') / pl.col('DAYS_BIRTH')).alias('EXT_SOURCE_1_BIRTH_RATIO'),\n",
    "        (pl.col('EXT_SOURCE_2') / pl.col('DAYS_BIRTH')).alias('EXT_SOURCE_2_BIRTH_RATIO'),\n",
    "        (pl.col('EXT_SOURCE_3') / pl.col('DAYS_BIRTH')).alias('EXT_SOURCE_3_BIRTH_RATIO'),\n",
    "\n",
    "        # Credit comparisons\n",
    "        (pl.col('AMT_CREDIT') - pl.col('AMT_GOODS_PRICE')).alias('CREDIT_MINUS_GOODS'),\n",
    "        (pl.col('AMT_CREDIT') / pl.col('AMT_GOODS_PRICE')).alias('CREDIT_DIV_GOODS'),\n",
    "        (pl.col('AMT_CREDIT') / pl.col('AMT_ANNUITY')).alias('CREDIT_DIV_ANNUITY'), \n",
    "        (pl.col('AMT_CREDIT') / pl.col('AMT_INCOME_TOTAL')).alias('CREDIT_DIV_INCOME'),\n",
    "\n",
    "        # Income relationships\n",
    "        (pl.col('AMT_INCOME_TOTAL') / 12.0 - pl.col('AMT_ANNUITY')).alias('INCOME_MONTHLY_MINUS_ANNUITY'),\n",
    "        (pl.col('AMT_INCOME_TOTAL') / pl.col('AMT_ANNUITY')).alias('INCOME_DIV_ANNUITY'),\n",
    "        (pl.col('AMT_INCOME_TOTAL') - pl.col('AMT_GOODS_PRICE')).alias('INCOME_MINUS_GOODS'),\n",
    "        (pl.col('AMT_INCOME_TOTAL') / pl.col('CNT_FAM_MEMBERS')).alias('INCOME_DIV_FAM_SIZE'),\n",
    "\n",
    "        # Price popularity\n",
    "        pl.col('AMT_GOODS_PRICE').is_in([225000, 450000, 675000, 900000]).cast(pl.Int32).alias('GOODS_PRICE_POPULAR_TIER_1'),\n",
    "        pl.col('AMT_GOODS_PRICE').is_in([1125000, 1350000, 1575000, 1800000, 2250000]).cast(pl.Int32).alias('GOODS_PRICE_POPULAR_TIER_2'),\n",
    "\n",
    "        # Car age ratios\n",
    "        (pl.col('OWN_CAR_AGE') / pl.col('DAYS_BIRTH')).alias('CAR_AGE_BIRTH_RATIO'),\n",
    "        (pl.col('OWN_CAR_AGE') / pl.col('DAYS_EMPLOYED')).alias('CAR_AGE_EMPLOYED_RATIO'),\n",
    "\n",
    "        # Phone change & employment timing\n",
    "        (pl.col('DAYS_LAST_PHONE_CHANGE') / pl.col('DAYS_BIRTH')).alias('PHONE_CHANGE_BIRTH_RATIO'),\n",
    "        (pl.col('DAYS_LAST_PHONE_CHANGE') / pl.col('DAYS_EMPLOYED')).alias('PHONE_CHANGE_EMPLOYED_RATIO'),\n",
    "        (pl.col('DAYS_EMPLOYED') - pl.col('DAYS_BIRTH')).alias('EMPLOYED_MINUS_BIRTH'),\n",
    "        (pl.col('DAYS_EMPLOYED') / pl.col('DAYS_BIRTH')).alias('EMPLOYED_BIRTH_RATIO'),\n",
    "    ])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    \"\"\"\n",
    "    Perform one-hot encoding on categorical columns of a pandas DataFrame.\n",
    "    Args:\n",
    "        df: pandas DataFrame.\n",
    "        nan_as_category: bool, whether to treat NaN as a separate category.\n",
    "    Returns:\n",
    "        Tuple of encoded DataFrame and list of new columns.\n",
    "    \"\"\"\n",
    " \n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'category']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "\n",
    "def one_hot_encoder_appl(df, nan_as_category=True):\n",
    "    \"\"\"\n",
    "    One-hot encode categorical columns in application data and build encoder map.\n",
    "    Args:\n",
    "        df: pandas DataFrame.\n",
    "        nan_as_category: bool, whether to treat NaN as a separate category.\n",
    "    Returns:\n",
    "        Tuple of encoded DataFrame, new columns, and encoder map.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    original_columns = list(df.columns)\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype.name == 'category']\n",
    "    print(\"Categorical columns:\", categorical_columns)\n",
    "\n",
    "    # Remove invalid gender entries\n",
    "    encoder_map = {}\n",
    "    for col in categorical_columns:\n",
    "        categories = df[col].cat.categories.tolist()\n",
    "        if nan_as_category:\n",
    "            categories = categories + ['NaN']\n",
    "        encoder_map[col] = categories\n",
    "\n",
    "\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "\n",
    "    return df, new_columns, encoder_map    \n",
    "\n",
    "def remove_features(df, remove_features):\n",
    "    \"\"\"\n",
    "    Remove specified features from the DataFrame.\n",
    "    Args:\n",
    "        df: DataFrame.\n",
    "        remove_features: list of column names to remove.\n",
    "    Returns:\n",
    "        DataFrame with specified columns dropped.\n",
    "    \"\"\"\n",
    "    return df.drop(remove_features)\n",
    "\n",
    "def apply_saved_encoding(df, encoder_map, nan_as_category=True):\n",
    "    \"\"\"\n",
    "    Apply saved encoding to DataFrame using encoder map.\n",
    "    Args:\n",
    "        df: pandas DataFrame.\n",
    "        encoder_map: dict mapping columns to categories.\n",
    "        nan_as_category: bool, whether to treat NaN as a separate category.\n",
    "    Returns:\n",
    "        Encoded DataFrame with numerical features.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    encoded_columns = []\n",
    "\n",
    "    for col, categories in encoder_map.items():\n",
    "        col_values = df[col].astype(str).fillna('NaN') if nan_as_category else df[col].astype(str)\n",
    "\n",
    "        for cat in categories:\n",
    "            encoded_col = f\"{col}_{cat}\"\n",
    "            encoded_series = (col_values == cat).astype(int)\n",
    "            encoded_columns.append(pd.DataFrame({encoded_col: encoded_series}))\n",
    "\n",
    "\n",
    "    encoded_df = pd.concat(encoded_columns, axis=1)\n",
    "\n",
    "    numerical_df = df.drop(columns=encoder_map.keys()).select_dtypes(include=['number'])\n",
    "\n",
    "    final_df = pd.concat([encoded_df, numerical_df], axis=1)\n",
    "\n",
    "    return final_df    \n",
    "\n",
    "\n",
    "def application_agg(df, nan_as_category = False):\n",
    "    \"\"\"\n",
    "    Aggregate and encode application data, clean and engineer features, and save encoder map.\n",
    "    Args:\n",
    "        df: polars DataFrame.\n",
    "        nan_as_category: bool, whether to treat NaN as a separate category.\n",
    "    Returns:\n",
    "        polars DataFrame with aggregated and encoded features.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = clean_applicans_data(df)\n",
    "    df = create_new_features_application(df)\n",
    "\n",
    "    df_application_pd = df.to_pandas()\n",
    "\n",
    "    df_application_pd[\"CODE_GENDER\"] = df_application_pd[\"CODE_GENDER\"].replace({'XNA': np.nan})\n",
    "    df_application_pd[\"NAME_INCOME_TYPE\"] = df_application_pd[\"NAME_INCOME_TYPE\"].replace({'Maternity leave': np.nan})\n",
    "    df_application_pd['NAME_FAMILY_STATUS'] = df_application_pd['NAME_FAMILY_STATUS'].replace({'Unknown': np.nan})\n",
    "\n",
    "    df_application, app_cat, encoder_map = one_hot_encoder_appl(df_application_pd, nan_as_category)\n",
    "\n",
    "    joblib.dump(encoder_map, \"../data/encoders/appl_encoder_map_v2.pkl\")\n",
    "\n",
    "    return pl.from_pandas(df_application)\n",
    "\n",
    "\n",
    "def application_agg_encoding(df:pl, encoder_map):\n",
    "    \"\"\"\n",
    "    Aggregate and encode application data using a provided encoder map.\n",
    "    Args:\n",
    "        df: polars DataFrame.\n",
    "        encoder_map: dict mapping columns to categories.\n",
    "    Returns:\n",
    "        polars DataFrame with encoded features.\n",
    "    \"\"\"\n",
    "\n",
    "    df = clean_applicans_data(df)\n",
    "    df = create_new_features_application(df)\n",
    "\n",
    "    df_application_pd = df.to_pandas()\n",
    "    df_application_pd[\"CODE_GENDER\"] = df_application_pd[\"CODE_GENDER\"].replace({'XNA': np.nan})\n",
    "    df_application_pd[\"NAME_INCOME_TYPE\"] = df_application_pd[\"NAME_INCOME_TYPE\"].replace({'Maternity leave': np.nan})\n",
    "    df_application_pd['NAME_FAMILY_STATUS'] = df_application_pd['NAME_FAMILY_STATUS'].replace({'Unknown': np.nan})\n",
    "\n",
    "    df_application = apply_saved_encoding(df_application_pd, encoder_map, nan_as_category=True)\n",
    "\n",
    "    return pl.from_pandas(df_application)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7f83cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_variance = [\n",
    "        'REGION_POPULATION_RELATIVE','BASEMENTAREA_AVG','YEARS_BEGINEXPLUATATION_AVG','COMMONAREA_AVG','LANDAREA_AVG','LIVINGAPARTMENTS_AVG',\n",
    "        'NONLIVINGAPARTMENTS_AVG','NONLIVINGAREA_AVG','BASEMENTAREA_MODE','YEARS_BEGINEXPLUATATION_MODE','COMMONAREA_MODE','LANDAREA_MODE','NONLIVINGAPARTMENTS_MODE',\n",
    "        'NONLIVINGAREA_MODE','BASEMENTAREA_MEDI','YEARS_BEGINEXPLUATATION_MEDI','COMMONAREA_MEDI','LANDAREA_MEDI','LIVINGAPARTMENTS_MEDI','NONLIVINGAPARTMENTS_MEDI',\n",
    "        'NONLIVINGAREA_MEDI', 'AMT_REQ_CREDIT_BUREAU_HOUR','AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK'\n",
    "       ]\n",
    "    \n",
    "weak_correlated =[\n",
    "        'OBS_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE','AMT_REQ_CREDIT_BUREAU_QRT',\n",
    "        'FLAG_DOCUMENT_15','FLAG_OWN_REALTY','NAME_TYPE_SUITE','REG_REGION_NOT_LIVE_REGION','HOUSETYPE_MODE',\n",
    "        'FLAG_DOCUMENT_2','FLAG_DOCUMENT_9','FLAG_DOCUMENT_11','WEEKDAY_APPR_PROCESS_START','FLAG_DOCUMENT_21',\n",
    "        'FLAG_DOCUMENT_17','LIVE_REGION_NOT_WORK_REGION','FLAG_DOCUMENT_4','FLAG_MOBIL','FLAG_EMAIL','FLAG_CONT_MOBILE',\n",
    "        'FLAG_DOCUMENT_5','FLAG_DOCUMENT_7','FLAG_DOCUMENT_12','FLAG_DOCUMENT_10','FLAG_DOCUMENT_19','FLAG_DOCUMENT_20',\n",
    "         'FLAG_EMP_PHONE'\n",
    "    ]\n",
    "REMOVE_APPLICATION_FEATURE =  low_variance + weak_correlated\n",
    "\n",
    "def main_application_aggragation(df:pl,train=True):\n",
    "    \"\"\"\n",
    "    Main aggregation pipeline for application data, switching between train and test encoding.\n",
    "    Args:\n",
    "        df: polars DataFrame.\n",
    "        train: bool, whether to use training pipeline.\n",
    "    Returns:\n",
    "        polars DataFrame with aggregated features.\n",
    "    \"\"\"\n",
    "    df = remove_features(df, REMOVE_APPLICATION_FEATURE)\n",
    "    if train:\n",
    "        appl_agg = application_agg(df, nan_as_category=True)\n",
    "    else:\n",
    "        encoder_map = joblib.load(\"../data/encoders/appl_encoder_map.pkl\")\n",
    "        appl_agg = application_agg_encoding(df, encoder_map)\n",
    "\n",
    "    return  appl_agg\n",
    "\n",
    "df_application_train = main_application_aggragation(application_train, train=False)\n",
    "df_application_test = main_application_aggragation(application_test,train=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59fdbc7",
   "metadata": {},
   "source": [
    "## 3. Aggregate External Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5cc242",
   "metadata": {},
   "source": [
    " ### 3.1 Bureau and Bureau_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00700e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bureu_balance(\n",
    "        df_bureau_b,\n",
    "        df_bureau,\n",
    "        nan_as_category = True):\n",
    "\n",
    "        \"\"\"\n",
    "        Merge bureau balance and bureau data, perform feature engineering and aggregation for modeling.\n",
    "        Args:\n",
    "            df_bureau_b: polars DataFrame for bureau balance.\n",
    "            df_bureau: polars DataFrame for bureau.\n",
    "            nan_as_category: bool, whether to treat NaN as a separate category.\n",
    "        Returns:\n",
    "            polars DataFrame with aggregated bureau features.\n",
    "        \"\"\"\n",
    "        \n",
    "        status_grouped = df_bureau_b.group_by('SK_ID_BUREAU').agg([\n",
    "        pl.col('STATUS').first().alias('Last_status'),\n",
    "        pl.col('STATUS').last().alias('First_status')\n",
    "        ])\n",
    "        df_bureau_b = df_bureau_b.join(status_grouped, on='SK_ID_BUREAU', how='left')\n",
    "\n",
    "        tmp_month = df_bureau_b.group_by('SK_ID_BUREAU').agg(\n",
    "        pl.col('MONTHS_BALANCE').last().abs().alias('Month')\n",
    "        )\n",
    "        df_bureau_b = df_bureau_b.join(tmp_month, on='SK_ID_BUREAU', how='left')\n",
    "\n",
    "        tmp_closed = df_bureau_b.filter(pl.col('STATUS') == 'C').group_by('SK_ID_BUREAU').agg(\n",
    "        pl.col('MONTHS_BALANCE').last().abs().alias('When_closed')\n",
    "        )\n",
    "        df_bureau_b = df_bureau_b.join(tmp_closed, on='SK_ID_BUREAU', how='left')\n",
    "\n",
    "        del tmp_closed, tmp_month\n",
    "\n",
    "        # Calculate Month_closed_to_end\n",
    "        df_bureau_b = df_bureau_b.with_columns(\n",
    "        (pl.col('Month') - pl.col('When_closed')).alias('Month_closed_to_end')\n",
    "        )\n",
    "        # DPD counts\n",
    "        for c in range(6):\n",
    "            tmp_dpd = df_bureau_b.filter(pl.col('STATUS') == str(c)).group_by('SK_ID_BUREAU').agg(\n",
    "                  pl.count('MONTHS_BALANCE').alias(f'DPD_{c}_cnt'))  \n",
    "            df_bureau_b = df_bureau_b.join(tmp_dpd, on='SK_ID_BUREAU', how='left')\n",
    "            df_bureau_b = df_bureau_b.with_columns(\n",
    "                  (pl.col(f'DPD_{c}_cnt') / pl.col('Month')).alias(f'DPD_{c} / Month')\n",
    "                  )\n",
    "        df_bureau_b = df_bureau_b.with_columns(\n",
    "              (pl.col('DPD_1_cnt') + pl.col('DPD_2_cnt') + pl.col('DPD_3_cnt') +\n",
    "              pl.col('DPD_4_cnt') + pl.col('DPD_5_cnt')).alias('Non_zero_DPD_cnt')\n",
    "        )\n",
    "        df_bureau_b_pd = df_bureau_b.to_pandas()\n",
    "        df_bureau_b, bureau_b_cat = one_hot_encoder(df_bureau_b_pd, nan_as_category)\n",
    "        df_bureau_b = pl.from_pandas(df_bureau_b)\n",
    "\n",
    "        aggregations = []\n",
    "        for col in df_bureau_b.columns:\n",
    "            if col == 'SK_ID_BUREAU':\n",
    "                   continue\n",
    "            if col in bureau_b_cat:\n",
    "                   aggregations.append(pl.col(col).mean().alias(f\"{col}_MEAN\"))\n",
    "            else:\n",
    "                aggregations.extend([\n",
    "                    pl.col(col).min().alias(f\"{col}_MIN\"),\n",
    "                    pl.col(col).max().alias(f\"{col}_MAX\"),\n",
    "                    pl.len().alias(f\"{col}_SIZE\")\n",
    "              ])\n",
    "\n",
    "        agg_df = df_bureau_b.group_by('SK_ID_BUREAU').agg(aggregations)\n",
    "        df_bureau_b_agg = agg_df.rename({col: col.upper() for col in agg_df.columns})\n",
    "\n",
    "        df_bureau = df_bureau.with_columns([\n",
    "            pl.when(pl.col('AMT_ANNUITY') > 0.8e8).then(None).otherwise(pl.col('AMT_ANNUITY')).alias('AMT_ANNUITY'),\n",
    "            pl.when(pl.col('AMT_CREDIT_SUM') > 3e8).then(None).otherwise(pl.col('AMT_CREDIT_SUM')).alias('AMT_CREDIT_SUM'),\n",
    "            pl.when(pl.col('AMT_CREDIT_SUM_DEBT') > 1e8).then(None).otherwise(pl.col('AMT_CREDIT_SUM_DEBT')).alias('AMT_CREDIT_SUM_DEBT'),\n",
    "            pl.when(pl.col('AMT_CREDIT_MAX_OVERDUE') > 0.8e8).then(None).otherwise(pl.col('AMT_CREDIT_MAX_OVERDUE')).alias('AMT_CREDIT_MAX_OVERDUE'),\n",
    "            pl.when(pl.col('DAYS_ENDDATE_FACT') < -10000).then(None).otherwise(pl.col('DAYS_ENDDATE_FACT')).alias('DAYS_ENDDATE_FACT'),\n",
    "            pl.when(\n",
    "                  (pl.col('DAYS_CREDIT_UPDATE') > 0) | (pl.col('DAYS_CREDIT_UPDATE') < -40000)\n",
    "            ).then(None).otherwise(pl.col('DAYS_CREDIT_UPDATE')).alias('DAYS_CREDIT_UPDATE'),\n",
    "            pl.when(pl.col('DAYS_CREDIT_ENDDATE') < -10000).then(None).otherwise(pl.col('DAYS_CREDIT_ENDDATE')).alias('DAYS_CREDIT_ENDDATE'),\n",
    "            ])\n",
    "    \n",
    "        df_bureau = df_bureau.filter(\n",
    "             pl.col('DAYS_ENDDATE_FACT') >= pl.col('DAYS_CREDIT')\n",
    "             )\n",
    "        drop_cols = [\n",
    "             'CREDIT_CURRENCY'\n",
    "            ]\n",
    "        df_bureau = df_bureau.drop(drop_cols)\n",
    "        \n",
    "        past_loan = (\n",
    "            df_bureau.group_by('SK_ID_CURR').agg(pl.count('SK_ID_BUREAU'))\n",
    "            .rename({'SK_ID_BUREAU': 'LOAN_COUNT_BUREAU'})\n",
    "            )\n",
    "            \n",
    "        df_bureau = df_bureau.with_columns(\n",
    "              (pl.col('AMT_CREDIT_SUM') - pl.col('AMT_CREDIT_SUM_DEBT')).alias('CREDIT_SUM_DIFF_DEBT'),\n",
    "              (pl.col('AMT_CREDIT_SUM') - pl.col('AMT_CREDIT_SUM_LIMIT')).alias('CREDIT_SUM_DIFF_LIMIT'),\n",
    "              (pl.col('AMT_CREDIT_SUM') - pl.col('AMT_CREDIT_SUM_OVERDUE')).alias('CREDIT_SUM_DIFF_OVERDUE'),\n",
    "              (pl.col('DAYS_CREDIT') - pl.col('CREDIT_DAY_OVERDUE')).alias('DAYS_DIFF_OVERDUE'),\n",
    "              (pl.col('DAYS_CREDIT') - pl.col('DAYS_CREDIT_ENDDATE')).alias('DAYS_DIFF_ENDDATE'),\n",
    "              (pl.col('DAYS_CREDIT') - pl.col('DAYS_ENDDATE_FACT')).alias('DAYS_DIFF_ENDDATE_FACT'),\n",
    "              (pl.col('DAYS_CREDIT_ENDDATE') - pl.col('DAYS_ENDDATE_FACT')).alias('ENDDATE_DIFF_FACT'),\n",
    "              (pl.col('DAYS_CREDIT_UPDATE') - pl.col('DAYS_CREDIT_ENDDATE')).alias('UPDATE_DIFF_ENDDATE')\n",
    "        )\n",
    "    \n",
    "        df_bureau_pd = df_bureau.to_pandas()\n",
    "        df_bureau, bureau_cat = one_hot_encoder(df_bureau_pd, nan_as_category)\n",
    "        df_bureau = pl.from_pandas(df_bureau)\n",
    "\n",
    "        df_bureau = df_bureau.join(df_bureau_b_agg, on='SK_ID_BUREAU', how='left')\n",
    "        df_bureau = df_bureau.drop('SK_ID_BUREAU')\n",
    " \n",
    "        categorical = bureau_cat + bureau_b_cat\n",
    " \n",
    "        aggregations_b = []\n",
    "        for col in df_bureau.columns:\n",
    "            if col == 'SK_ID_CURR':\n",
    "                continue\n",
    "            if col in categorical:\n",
    "                aggregations_b.append(pl.col(col).mean().alias(f\"{col}_MEAN_\"))\n",
    "        \n",
    "        aggregations_b.extend( [\n",
    "            pl.col('DAYS_CREDIT').min().alias('DAYS_CREDIT_MIN'),\n",
    "            pl.col('DAYS_CREDIT').max().alias('DAYS_CREDIT_MAX'),\n",
    "            pl.col('DAYS_CREDIT').mean().alias('DAYS_CREDIT_MEAN'),\n",
    "            pl.col('DAYS_CREDIT').var().alias('DAYS_CREDIT_VAR'),\n",
    "            pl.col('DAYS_CREDIT_ENDDATE').min().alias('DAYS_CREDIT_ENDDATE_MIN'),\n",
    "            pl.col('DAYS_CREDIT_ENDDATE').max().alias('DAYS_CREDIT_ENDDATE_MAX'),\n",
    "            pl.col('DAYS_CREDIT_ENDDATE').mean().alias('DAYS_CREDIT_ENDDATE_MEAN'),\n",
    "            pl.col('AMT_CREDIT_MAX_OVERDUE').mean().alias('B_AMT_CREDIT_MAX_OVERDUE_MEAN'),\n",
    "            pl.col('AMT_CREDIT_SUM_DEBT').mean().alias('AMT_CREDIT_SUM_DEBT_MEAN'),\n",
    "            pl.col('AMT_CREDIT_SUM_DEBT').sum().alias('AMT_CREDIT_SUM_DEBT_SUM'),\n",
    "            pl.col('AMT_CREDIT_SUM_OVERDUE').mean().alias('AMT_CREDIT_SUM_OVERDUE_MEAN'),\n",
    "            pl.col('AMT_CREDIT_SUM_LIMIT').mean().alias('AMT_CREDIT_SUM_LIMIT_MEAN'),\n",
    "            pl.col('AMT_CREDIT_SUM_LIMIT').sum().alias('AMT_CREDIT_SUM_LIMIT_SUM'),\n",
    "            pl.col('MONTHS_BALANCE_MIN').min().alias('B_MONTHS_BALANCE_MIN_MIN'),\n",
    "            pl.col('MONTHS_BALANCE_MAX').max().alias('B_MONTHS_BALANCE_MAX_MAX'),\n",
    "            pl.col('MONTHS_BALANCE_SIZE').mean().alias('MONTHS_BALANCE_SIZE_MEAN'),\n",
    "            pl.col('MONTHS_BALANCE_SIZE').sum().alias('MONTHS_BALANCE_SIZE_SUM'),\n",
    "            pl.col('CREDIT_SUM_DIFF_DEBT').mean().alias('CREDIT_SUM_DIFF_DEBT_MEAN'),\n",
    "            pl.col('CREDIT_SUM_DIFF_LIMIT').mean().alias('CREDIT_SUM_DIFF_LIMIT_MEAN'),\n",
    "            pl.col('CREDIT_SUM_DIFF_OVERDUE').mean().alias('CREDIT_SUM_DIFF_OVERDUE_MEAN'),\n",
    "            pl.col('DAYS_DIFF_OVERDUE').mean().alias('DAYS_DIFF_OVERDUE_MEAN'),\n",
    "            pl.col('DAYS_DIFF_ENDDATE').mean().alias('DAYS_DIFF_ENDDATE_MEAN'),\n",
    "            pl.col('DAYS_DIFF_ENDDATE_FACT').mean().alias('DAYS_DIFF_ENDDATE_FACT_MEAN'),\n",
    "            pl.col('ENDDATE_DIFF_FACT').mean().alias('ENDDATE_DIFF_FACT_MEAN'),\n",
    "            pl.col('UPDATE_DIFF_ENDDATE').mean().alias('UPDATE_DIFF_ENDDATE_MEAN')\n",
    "  \n",
    "        ])        \n",
    "        df_bureau_agg = df_bureau.group_by('SK_ID_CURR').agg(aggregations_b)\n",
    "        df_bureau_agg = df_bureau_agg.join(past_loan, on='SK_ID_CURR', how='left')\n",
    "        df_bureau_agg = df_bureau_agg.rename({col: col.upper() for col in df_bureau_agg.columns})\n",
    "        \n",
    "        return  df_bureau_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2fb7a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before reduction: 73.62 MB\n",
      "Initial data types: Counter({Float64: 26, Float32: 15, Int16: 2, Int8: 2, UInt32: 2, Int32: 1})\n",
      "Size after reduction: 47.02 MB\n",
      "Final data types: Counter({Float32: 41, Int16: 2, Int8: 2, UInt32: 2, Int32: 1})\n"
     ]
    }
   ],
   "source": [
    "df_bureau_agg = reduce_memory_usage_pl(merge_bureu_balance(bureau_bal, bureau))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ed6cc",
   "metadata": {},
   "source": [
    "- ### 3.2 Previous Appplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c94fbb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def previous_application(df_prev, nan_as_category= True):\n",
    "    \"\"\"\n",
    "    Aggregate previous application data, engineer new features, and encode categorical columns.\n",
    "    Args:\n",
    "        df_prev: polars DataFrame for previous applications.\n",
    "        nan_as_category: bool, whether to treat NaN as a separate category.\n",
    "    Returns:\n",
    "        polars DataFrame with aggregated previous application features.\n",
    "    \"\"\"\n",
    "    df_prev = df_prev.with_columns(\n",
    "          pl.when(pl.col('AMT_CREDIT') > 6_000_000).then(None).otherwise(pl.col('AMT_CREDIT')).alias('AMT_CREDIT')\n",
    "          )\n",
    "\n",
    "    replace_365243 = [\n",
    "        'DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE', 'DAYS_LAST_DUE_1ST_VERSION',\n",
    "        'DAYS_LAST_DUE', 'DAYS_TERMINATION'\n",
    "    ]\n",
    "    drop_columns = [\n",
    "        'SELLERPLACE_AREA',\n",
    "        'RATE_INTEREST_PRIMARY',\n",
    "        'NFLAG_INSURED_ON_APPROVAL',\n",
    "        'NAME_SELLER_INDUSTRY'\n",
    "    ]\n",
    "\n",
    "    df_prev = df_prev.drop(drop_columns)\n",
    "\n",
    "    for col in replace_365243:\n",
    "        df_prev = df_prev.with_columns(\n",
    "            pl.when(pl.col(col) == 365243).then(None).otherwise(pl.col(col)).alias(col)\n",
    "        )\n",
    "    \n",
    "    df_prev = df_prev.with_columns([\n",
    "        (pl.col('AMT_APPLICATION') / pl.col('AMT_CREDIT')).alias('PREV_APP_CREDIT_PERC'),\n",
    "        (pl.col('AMT_APPLICATION') - pl.col('AMT_CREDIT')).alias('PREV_AMT_DECLINED'),\n",
    "        (pl.col('AMT_CREDIT') - pl.col('AMT_GOODS_PRICE')).alias('PREV_AMT_CREDIT_GOODS_DIF'),\n",
    "        (pl.col('CNT_PAYMENT') *  pl.col('AMT_ANNUITY') - pl.col('AMT_CREDIT')).alias('AMT_INTEREST'),\n",
    "        ])\n",
    "\n",
    "    df_prev_pd = df_prev.to_pandas()\n",
    "    df_prev, prev_cat = one_hot_encoder(df_prev_pd, nan_as_category)\n",
    "    df_prev = pl.from_pandas(df_prev)\n",
    "\n",
    "    cat_agg = [pl.col(col).mean().alias(f\"{col}_MEAN\") for col in df_prev[prev_cat].columns]            \n",
    "    \n",
    "    num_agg= [\n",
    "        pl.col('AMT_ANNUITY').min().alias('AMT_ANNUITY_MIN'),\n",
    "        pl.col('AMT_ANNUITY').max().alias('AMT_ANNUITY_MAX'),\n",
    "        pl.col('AMT_ANNUITY').mean().alias('AMT_ANNUITY_MEAN'),\n",
    "        pl.col('AMT_CREDIT').mean().alias('AMT_CREDIT_MEAN'),\n",
    "        pl.col('PREV_APP_CREDIT_PERC').min().alias('APP_CREDIT_PERC_MIN'),\n",
    "        pl.col('PREV_APP_CREDIT_PERC').max().alias('APP_CREDIT_PERC_MAX'),\n",
    "        pl.col('PREV_APP_CREDIT_PERC').mean().alias('APP_CREDIT_PERC_MEAN'),\n",
    "        pl.col('PREV_APP_CREDIT_PERC').var().alias('APP_CREDIT_PERC_VAR'),\n",
    "        pl.col('PREV_AMT_DECLINED').min().alias('PREV_AMT_DECLINED_MIN'),\n",
    "        pl.col('PREV_AMT_DECLINED').max().alias('PREV_AMT_DECLINED_MAX'),\n",
    "        pl.col('PREV_AMT_DECLINED').mean().alias('PREV_AMT_DECLINED_MEAN'),\n",
    "        pl.col('AMT_GOODS_PRICE').mean().alias('PREV_AMT_GOODS_PRICE_MEAN'), \n",
    "        pl.col('HOUR_APPR_PROCESS_START').min().alias('HOUR_APPR_PROCESS_START_MIN'),\n",
    "        pl.col('HOUR_APPR_PROCESS_START').max().alias('HOUR_APPR_PROCESS_START_MAX'),\n",
    "        pl.col('HOUR_APPR_PROCESS_START').mean().alias('HOUR_APPR_PROCESS_START_MEAN'),\n",
    "        pl.col('RATE_DOWN_PAYMENT').min().alias('RATE_DOWN_PAYMENT_MIN'),\n",
    "        pl.col('RATE_DOWN_PAYMENT').max().alias('RATE_DOWN_PAYMENT_MAX'),\n",
    "        pl.col('RATE_DOWN_PAYMENT').mean().alias('RATE_DOWN_PAYMENT_MEAN'),\n",
    "        pl.col('DAYS_DECISION').min().alias('DAYS_DECISION_MIN'),\n",
    "        pl.col('DAYS_DECISION').max().alias('DAYS_DECISION_MAX'),\n",
    "        pl.col('DAYS_DECISION').mean().alias('DAYS_DECISION_MEAN'),\n",
    "        pl.col('CNT_PAYMENT').mean().alias('PREV_CNT_PAYMENT_MEAN'),\n",
    "        pl.col('CNT_PAYMENT').sum().alias('PREV_CNT_PAYMENT_SUM'),\n",
    "    ]\n",
    "    df_prev_agg = df_prev.group_by('SK_ID_CURR').agg(cat_agg + num_agg)\n",
    "    df_prev_agg = df_prev_agg.rename({col: col.upper() for col in df_prev_agg.columns})\n",
    "\n",
    "    return df_prev_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02c3cbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before reduction: 410.89 MB\n",
      "Initial data types: Counter({Float64: 149, Float32: 17, Int8: 2, Int16: 2, Int32: 1})\n",
      "Size after reduction: 218.29 MB\n",
      "Final data types: Counter({Float32: 166, Int8: 2, Int16: 2, Int32: 1})\n"
     ]
    }
   ],
   "source": [
    "df_previous_agg =  reduce_memory_usage_pl(previous_application(prev_app_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200eed80",
   "metadata": {},
   "source": [
    "- ### 3.3 Installments aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb81715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def installments_payments_agg(df_ins, nan_as_category = True):\n",
    "    \"\"\"\n",
    "    Aggregate installment payments data, engineer new features, and encode categorical columns.\n",
    "    Args:\n",
    "        df_ins: polars DataFrame for installment payments.\n",
    "        nan_as_category: bool, whether to treat NaN as a separate category.\n",
    "    Returns:\n",
    "        polars DataFrame with aggregated installment payment features.\n",
    "    \"\"\"\n",
    "  \n",
    "    df_ins = df_ins.with_columns(\n",
    "        pl.when(pl.col('NUM_INSTALMENT_VERSION') > 60).then(None).otherwise(pl.col('NUM_INSTALMENT_VERSION')).alias('NUM_INSTALMENT_VERSION')\n",
    "        )\n",
    "    \n",
    "    df_ins = df_ins.with_columns(\n",
    "          pl.when(pl.col('DAYS_ENTRY_PAYMENT') < -4000).then(None).otherwise(pl.col('DAYS_ENTRY_PAYMENT')).alias('DAYS_ENTRY_PAYMENT')\n",
    "          )\n",
    "    df_ins = df_ins.with_columns([\n",
    "        (pl.col('DAYS_ENTRY_PAYMENT') - pl.col('AMT_INSTALMENT')).alias('DAYS_PAST_DUE'),\n",
    "        (pl.col('AMT_INSTALMENT') - pl.col('DAYS_ENTRY_PAYMENT')).alias('DAYS_BEFORE_DUE'),\n",
    "        (pl.col('AMT_PAYMENT') / pl.col('AMT_INSTALMENT')).alias('PAYMENT_RATIO'),\n",
    "        (pl.col('DAYS_INSTALMENT') / pl.col('DAYS_ENTRY_PAYMENT')).alias('DAYS_PAYMENT_RATIO'),\n",
    "        (pl.col('AMT_INSTALMENT') - pl.col('AMT_PAYMENT')).alias('PAYMENT_DIFF')\n",
    "        ])\n",
    "    # Ensure only positive values (replace negatives with 0)\n",
    "    df_ins = df_ins.with_columns([\n",
    "        pl.when(pl.col('DAYS_PAST_DUE') > 0).then(pl.col('DAYS_PAST_DUE')).otherwise(0).alias('DAYS_PAST_DUE'),\n",
    "        pl.when(pl.col('DAYS_BEFORE_DUE') > 0).then(pl.col('DAYS_BEFORE_DUE')).otherwise(0).alias('DAYS_BEFORE_DUE')\n",
    "        ])\n",
    "\n",
    "    num_agg = [\n",
    "        pl.col('DAYS_PAST_DUE').max().alias('DAYS_PAST_DUE_MAX'),\n",
    "        pl.col('DAYS_PAST_DUE').mean().alias('DAYS_PAST_DUE_MEAN'),\n",
    "        pl.col('DAYS_PAST_DUE').sum().alias('DAYS_PAST_DUE_SUM'),\n",
    "\n",
    "        pl.col('DAYS_BEFORE_DUE').max().alias('DAYS_BEFORE_DUE_MAX'),\n",
    "        pl.col('DAYS_BEFORE_DUE').mean().alias('DAYS_BEFORE_DUE_MEAN'),\n",
    "        pl.col('DAYS_BEFORE_DUE').sum().alias('DAYS_BEFORE_DUE_SUM'),\n",
    "\n",
    "        pl.col('PAYMENT_RATIO').max().alias('PAYMENT_RATIO_MAX'),\n",
    "        pl.col('PAYMENT_RATIO').mean().alias('PAYMENT_RATIO_MEAN'),\n",
    "        pl.col('PAYMENT_RATIO').sum().alias('PAYMENT_RATIO_SUM'),\n",
    "        pl.col('PAYMENT_RATIO').var().alias('PAYMENT_RATIO_VAR'),\n",
    "\n",
    "        pl.col('DAYS_PAYMENT_RATIO').max().alias('DAYS_PAYMENT_RATIO_MAX'),\n",
    "        pl.col('DAYS_PAYMENT_RATIO').mean().alias('DAYS_PAYMENT_RATIO_MEAN'),\n",
    "        pl.col('DAYS_PAYMENT_RATIO').sum().alias('DAYS_PAYMENT_RATIO_SUM'),\n",
    "        pl.col('DAYS_PAYMENT_RATIO').var().alias('DAYS_PAYMENT_RATIO_VAR'),\n",
    "\n",
    "        pl.col('PAYMENT_DIFF').max().alias('PAYMENT_DIFF_MAX'),\n",
    "        pl.col('PAYMENT_DIFF').mean().alias('PAYMENT_DIFF_MEAN'),\n",
    "        pl.col('PAYMENT_DIFF').sum().alias('PAYMENT_DIFF_SUM'),\n",
    "        pl.col('PAYMENT_DIFF').var().alias('PAYMENT_DIFF_VAR'),\n",
    "\n",
    "        pl.col('AMT_INSTALMENT').mean().alias('AMT_INSTALMENT_MEAN'),\n",
    "        pl.col('AMT_PAYMENT').mean().alias('AMT_PAYMENT_MEAN'),\n",
    "     \n",
    "        pl.col('DAYS_ENTRY_PAYMENT').max().alias('DAYS_ENTRY_PAYMENT_MAX'),\n",
    "        pl.col('DAYS_ENTRY_PAYMENT').mean().alias('DAYS_ENTRY_PAYMENT_MEAN'),\n",
    "        pl.col('DAYS_ENTRY_PAYMENT').sum().alias('DAYS_ENTRY_PAYMENT_SUM'),\n",
    "        ]\n",
    "    \n",
    "    df_ins_pd = df_ins.to_pandas()\n",
    "    df_ins, ins_cat = one_hot_encoder(df_ins_pd, nan_as_category)\n",
    "    df_ins = pl.from_pandas(df_ins)\n",
    "\n",
    "    cat_agg = [pl.col(col).mean().alias(f\"{col}_MEAN\") for col in df_ins[ins_cat].columns]   \n",
    "\n",
    "    df_ins_agg = df_ins.group_by('SK_ID_CURR').agg(num_agg) \n",
    "    df_ins_agg = df_ins_agg.rename({col: col.upper() for col in df_ins_agg.columns})\n",
    "\n",
    "    return df_ins_agg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ee0c053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before reduction: 31.24 MB\n",
      "Initial data types: Counter({Float32: 23, Int32: 1})\n",
      "Size after reduction: 31.24 MB\n",
      "Final data types: Counter({Float32: 23, Int32: 1})\n"
     ]
    }
   ],
   "source": [
    "df_installments_agg = reduce_memory_usage_pl(installments_payments_agg(inst_pay_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc29cd76",
   "metadata": {},
   "source": [
    "- ### 3.4 POS_CASH aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b67d5cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_cash_agg(df_pos, nan_as_category = True):\n",
    "    \"\"\"\n",
    "    Aggregate POS cash balance data, engineer new features, and encode categorical columns.\n",
    "    Args:\n",
    "        df_pos: polars DataFrame for POS cash balance.\n",
    "        nan_as_category: bool, whether to treat NaN as a separate category.\n",
    "    Returns:\n",
    "        polars DataFrame with aggregated POS cash features.\n",
    "    \"\"\"\n",
    "\n",
    "    df_pos = df_pos.with_columns(\n",
    "          pl.when(pl.col('CNT_INSTALMENT_FUTURE') > 60).then(None).otherwise(pl.col('CNT_INSTALMENT_FUTURE')).alias('CNT_INSTALMENT_FUTURE')\n",
    "          )\n",
    "    df_pos = df_pos.with_columns([\n",
    "        (pl.col('SK_DPD') / pl.col('SK_DPD_DEF')).alias('APP_CREDIT_PERC'),\n",
    "        (pl.col('CNT_INSTALMENT') + pl.col('CNT_INSTALMENT_FUTURE')).alias('TOTAL_TERM'),\n",
    "        (pl.col('CNT_INSTALMENT') > pl.col('CNT_INSTALMENT_FUTURE')).alias('pos_CMT_INSTALMEN_MORE_FUTURE')\n",
    "        ])\n",
    "\n",
    "    df_pos_pd = df_pos.to_pandas()\n",
    "    df_pos, pos_cat = one_hot_encoder(df_pos_pd, nan_as_category)\n",
    "    df_pos = pl.from_pandas(df_pos)\n",
    "  \n",
    "    num_agg = [\n",
    "        pl.col('MONTHS_BALANCE').max().alias('MONTHS_BALANCE_MAX'),\n",
    "        pl.col('MONTHS_BALANCE').mean().alias('MONTHS_BALANCE_MEAN'),\n",
    "        pl.col('MONTHS_BALANCE').len().alias('MONTHS_BALANCE_SIZE'),\n",
    "\n",
    "        pl.col('SK_DPD').max().alias('SK_DPD_MAX'),\n",
    "        pl.col('SK_DPD').mean().alias('SK_DPD_MEAN'),\n",
    "        \n",
    "        pl.col('SK_DPD_DEF').max().alias('SK_DPD_DEF_MAX'),\n",
    "        pl.col('SK_DPD_DEF').mean().alias('SK_DPD_DEF_MEAN'),\n",
    "    ]\n",
    "\n",
    "    cat_agg = [pl.col(col).mean().alias(f\"{col}_MEAN\") for col in df_pos[pos_cat].columns]   \n",
    "\n",
    "    df_pos_agg = df_pos.group_by('SK_ID_CURR').agg(cat_agg + num_agg) \n",
    "    df_pos_agg = df_pos_agg.rename({col: col.upper() for col in df_pos_agg.columns})\n",
    "\n",
    "    pos_cash_count = df_pos_agg.group_by('SK_ID_CURR').len()\n",
    "    df_pos_agg = df_pos_agg.join(pos_cash_count, on='SK_ID_CURR', how='left')\n",
    "    \n",
    "    return  df_pos_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ae5d57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before reduction: 38.92 MB\n",
      "Initial data types: Counter({Float64: 13, UInt32: 2, Int16: 2, Int32: 1, Int8: 1})\n",
      "Size after reduction: 22.19 MB\n",
      "Final data types: Counter({Float32: 13, UInt32: 2, Int16: 2, Int32: 1, Int8: 1})\n"
     ]
    }
   ],
   "source": [
    "df_pos_cash_agg = reduce_memory_usage_pl(pos_cash_agg(pos_cash))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800a6eee",
   "metadata": {},
   "source": [
    "- ### 3.5  Credit Card Balance Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0db9c817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_card_balance_agg(df_card, nan_as_category = True):\n",
    "    \"\"\"\n",
    "    Aggregate credit card balance data, engineer new features, and encode categorical columns.\n",
    "    Args:\n",
    "        df_card: polars DataFrame for credit card balance.\n",
    "        nan_as_category: bool, whether to treat NaN as a separate category.\n",
    "    Returns:\n",
    "        polars DataFrame with aggregated credit card balance features.\n",
    "    \"\"\"\n",
    "\n",
    "    df_card = df_card.with_columns([\n",
    "        pl.when(pl.col('AMT_PAYMENT_CURRENT') > 4_000_000)\n",
    "        .then(None)\n",
    "        .otherwise(pl.col('AMT_PAYMENT_CURRENT'))\n",
    "        .alias('AMT_PAYMENT_CURRENT'),\n",
    "\n",
    "        pl.when(pl.col('AMT_CREDIT_LIMIT_ACTUAL') > 1_000_000)\n",
    "        .then(None)\n",
    "        .otherwise(pl.col('AMT_CREDIT_LIMIT_ACTUAL'))\n",
    "        .alias('AMT_CREDIT_LIMIT_ACTUAL')\n",
    "    ])\n",
    "                                    \n",
    "    df_card = df_card.with_columns([pl.concat_list([pl.col(col).is_null().cast(pl.Int32) for col in df_card.columns]).list.sum().alias('card missing')])\n",
    "\n",
    "    df_card = df_card.with_columns([\n",
    "        (pl.col('AMT_DRAWINGS_ATM_CURRENT') +\n",
    "        pl.col('AMT_DRAWINGS_CURRENT') +\n",
    "        pl.col('AMT_DRAWINGS_OTHER_CURRENT') +\n",
    "        pl.col('AMT_DRAWINGS_POS_CURRENT')).alias('CARD_AMT_DRAWING_SUM'),\n",
    "\n",
    "        # Balance-to-limit ratio\n",
    "        (pl.col('AMT_BALANCE') / (pl.col('AMT_CREDIT_LIMIT_ACTUAL') + 0.00001)).alias('CARD_BALANCE_LIMIT_RATIO'),\n",
    "\n",
    "        # Total drawing count\n",
    "        (pl.col('CNT_DRAWINGS_ATM_CURRENT') +\n",
    "        pl.col('CNT_DRAWINGS_CURRENT') +\n",
    "        pl.col('CNT_DRAWINGS_OTHER_CURRENT') +\n",
    "        pl.col('CNT_DRAWINGS_POS_CURRENT') +\n",
    "        pl.col('CNT_INSTALMENT_MATURE_CUM')).alias('CARD_CNT_DRAWING_SUM'),\n",
    "\n",
    "        (pl.col('AMT_PAYMENT_CURRENT') / (pl.col('AMT_INST_MIN_REGULARITY') + 0.0001)).alias('CARD_MIN_PAYMENT_RATIO'),\n",
    "        (pl.col('AMT_PAYMENT_CURRENT') - pl.col('AMT_INST_MIN_REGULARITY')).alias('CARD_PAYMENT_MIN_DIFF'),\n",
    "        (pl.col('AMT_PAYMENT_TOTAL_CURRENT') / (pl.col('AMT_INST_MIN_REGULARITY') + 0.00001)).alias('CARD_MIN_PAYMENT_TOTAL_RATIO'),\n",
    "        (pl.col('AMT_PAYMENT_TOTAL_CURRENT') - pl.col('AMT_INST_MIN_REGULARITY')).alias('CARD_PAYMENT_MIN_TOTAL_DIFF'),\n",
    "        (pl.col('AMT_TOTAL_RECEIVABLE') - pl.col('AMT_RECEIVABLE_PRINCIPAL')).alias('CARD_INTEREST_RECEIVABLE'),\n",
    "        (pl.col('SK_DPD') / (pl.col('SK_DPD_DEF') + 0.00001)).alias('CARD_DPD_RATIO')\n",
    "    ])\n",
    "\n",
    "    \n",
    "    drop_columns = [\n",
    "       'AMT_TOTAL_RECEIVABLE',\n",
    "       'AMT_PAYMENT_TOTAL_CURRENT',\n",
    "       'AMT_RECEIVABLE_PRINCIPAL',\n",
    "       'AMT_RECIVABLE',\n",
    "       'AMT_DRAWINGS_ATM_CURRENT',\n",
    "       'AMT_DRAWINGS_CURRENT',\n",
    "    ]\n",
    "\n",
    "    df_card= df_card.drop(drop_columns)\n",
    "\n",
    "    df_card_pd = df_card.to_pandas()\n",
    "    df_card, card_cat = one_hot_encoder(df_card_pd, nan_as_category)\n",
    "    df_card = pl.from_pandas(df_card)\n",
    "\n",
    "    aggregations = []\n",
    "    for col in df_card.columns:\n",
    "        if col == 'SK_ID_PREV' or col == 'SK_ID_CURR':\n",
    "                continue\n",
    "        if col in card_cat:\n",
    "                aggregations.append(pl.col(col).mean().alias(f\"card_{col}_MEAN+\"))\n",
    "        else:\n",
    "            aggregations.extend([\n",
    "                pl.col(col).min().alias(f\"card_{col}_MIN\"),\n",
    "                pl.col(col).max().alias(f\"card_{col}_MAX\"),\n",
    "                pl.col(col).len().alias(f\"card_{col}_SIZE\"),\n",
    "                pl.col(col).mean().alias(f\"card_{col}_MEAN_\")\n",
    "        ])\n",
    "\n",
    "    df_card_agg = df_card.group_by('SK_ID_CURR').agg(aggregations)\n",
    "    df_card_agg = df_card_agg.rename({col: col.upper() for col in df_card_agg.columns})\n",
    "  \n",
    "    return df_card_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f8e2a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before reduction: 47.61 MB\n",
      "Initial data types: Counter({Float32: 51, UInt32: 24, Float64: 19, Int16: 6, Int32: 3, Int8: 2})\n",
      "Size after reduction: 39.41 MB\n",
      "Final data types: Counter({Float32: 70, UInt32: 24, Int8: 5, Int16: 5, Int32: 1})\n"
     ]
    }
   ],
   "source": [
    "df_credit_card_agg = reduce_memory_usage_pl(credit_card_balance_agg(card_balance))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb310a1",
   "metadata": {},
   "source": [
    "## 4. Merge All Aggregated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74327ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on SK_ID_CURR\n",
    "df_merged_train = df_application_train \\\n",
    "    .join(df_bureau_agg, on=\"SK_ID_CURR\", how=\"left\") \\\n",
    "    .join(df_previous_agg, on=\"SK_ID_CURR\", how=\"left\") \\\n",
    "    .join(df_installments_agg, on=\"SK_ID_CURR\", how=\"left\") \\\n",
    "    .join(df_credit_card_agg, on=\"SK_ID_CURR\", how=\"left\") \\\n",
    "    .join(df_pos_cash_agg, on=\"SK_ID_CURR\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f04e0055",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_test = df_application_test \\\n",
    "    .join(df_bureau_agg, on=\"SK_ID_CURR\", how=\"left\") \\\n",
    "    .join(df_previous_agg, on=\"SK_ID_CURR\", how=\"left\") \\\n",
    "    .join(df_installments_agg, on=\"SK_ID_CURR\", how=\"left\") \\\n",
    "    .join(df_credit_card_agg, on=\"SK_ID_CURR\", how=\"left\") \\\n",
    "    .join(df_pos_cash_agg, on=\"SK_ID_CURR\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc71e49",
   "metadata": {},
   "source": [
    "## 5. Final Checks & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88174ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bureau_agg.write_parquet(\"../data/cache/bureau_balance_agg.parquet\")\n",
    "# df_previous_agg.write_parquet(\"../data/cache/previous_application_agg.parquet\")\n",
    "# df_installments_agg.write_parquet(\"../data/cache/installments_agg.parquet\")\n",
    "# df_pos_cash_agg.write_parquet(\"../data/cache/pos_cash_agg.parquet\")\n",
    "# df_credit_card_agg.write_parquet(\"../data/cache/credit_card_agg.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9082a441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307500, 580)\n",
      "(48744, 579)\n"
     ]
    }
   ],
   "source": [
    "print(df_merged_train.shape)\n",
    "print(df_merged_test.shape)\n",
    "df_merged_train.write_parquet(\"../data/processed/data_aggregated_train.parquet\")\n",
    "df_merged_test.write_parquet(\"../data/processed/data_aggregated_test_v1.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anplien-ds-v2-5-3-4-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
